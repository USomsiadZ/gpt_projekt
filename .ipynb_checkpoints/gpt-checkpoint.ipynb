{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf10e26-4608-4cbf-a915-1c76407abbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: fastapi in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (0.128.0)\n",
      "Requirement already satisfied: uvicorn in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (0.39.0)\n",
      "Requirement already satisfied: nest-asyncio in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from triton==3.4.0->torch) (73.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from triton==3.4.0->torch) (8.5.0)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from fastapi) (0.49.3)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from fastapi) (2.12.5)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from fastapi) (0.0.4)\n",
      "Requirement already satisfied: click>=7.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from pydantic>=2.7.0->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from pydantic>=2.7.0->fastapi) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from pydantic>=2.7.0->fastapi) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (1.2.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from importlib-metadata->triton==3.4.0->torch) (3.20.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wsl/.conda/envs/sage/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pobranie bibliotek oraz modelu\n",
    "!pip install transformers torch fastapi uvicorn nest-asyncio\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name=\"gpt2\"\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, resume_download=True)#tokenizer zamienia słowo na ID tokenów\n",
    "tokenizer.pad_token=tokenizer.eos_token #zamienia token dopełnienia PAD na token końca sekwencji EOS |GPT2 nie posiada własnego tokena pad\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, resume_download=True)\n",
    "model.eval() #tryb ewaluacji\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffd5a9-cea4-4034-9f97-8d513c441956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [2901]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id tokenów w macierzy embedding:tensor([[10919,   318,   262,  4252]])\n",
      "Attention mask:tensor([[1, 1, 1, 1]])\n",
      "Pokazuje dla których tokenów liczyć prawdopodobieństwo a dla których nie\n",
      "INFO:     127.0.0.1:63039 - \"GET /generate?prompt=what%20is%20the%20sun HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI #framework do tworzenia REST api (m.in. obsługuje żądania HTTP) | FastAPI tworzy aplikacje serwerową\n",
    "import nest_asyncio #pozwala uruchomić asynchroniczny kod w środowisku np. Jupyter\n",
    "import uvicorn #Serwer ASGI (Asynchronous Server Gateway Interface), czyli serwer, który obsługuje FastAPI. To on nasłuchuje na porcie i odpowiada na żądania HTTP.\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()  # potrzebne w Jupyterze\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"status\": \"API GPT-2 działa\", \"endpoint\": \"/generate\"}\n",
    "\n",
    "@app.get(\"/info\")\n",
    "def info():\n",
    "    return {\n",
    "        \"model\": \"gpt2\",\n",
    "        \"vocab_size\": model.config.vocab_size,\n",
    "        \"embedding_dim\": model.config.n_embd\n",
    "    }\n",
    "\n",
    "@app.get(\"/generate\")\n",
    "def generate_text(prompt: str, max_length: int = 100):\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,#tekst od użytkownika\n",
    "        return_tensors=\"pt\",#zwróć wynik jako tensor PyTorch\n",
    "        padding=True #dopasowuje długość sekwencji dodając puste tokeny\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(device)#id tokenów które trafiają do modelu\n",
    "    print(f\"Id tokenów w macierzy embedding:{input_ids}\")\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    print(f\"Attention mask:{attention_mask}\\nPokazuje dla których tokenów liczyć prawdopodobieństwo a dla których nie\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,#model bierze tokeny\n",
    "        attention_mask=attention_mask,#przewiduje kolejne tokeny\n",
    "        max_length=max_length,\n",
    "        do_sample=True,#losuje kolejne tokeny if true, if False : zawsze najwieksze prawdopodobienstwo\n",
    "        pad_token_id=tokenizer.eos_token_id #informuje model czym jest padding (czy dany token ignorować czy nie)\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)#zamiana wektorów na tekst\n",
    "    return {\"generated_text\": text}\n",
    "\n",
    "\n",
    "config=uvicorn.Config(app=app,host=\"127.0.0.1\",port=5000,log_level=\"info\")\n",
    "server=uvicorn.Server(config=config)\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(server.serve())\n",
    "#aby uruchomić wpisz http://127.0.0.1:5000/generate?prompt=hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b302fc-0433-40fa-a425-dcaa269f98c7",
   "metadata": {},
   "source": [
    "**Tokenizacja** to proces zamiany tekstu na sekwencję identyfikatorów tokenów (input_ids), które indeksują wektory embeddingów, oraz maski uwagi (attention_mask), która informuje model, które tokeny powinny być brane pod uwagę podczas obliczeń.\n",
    "\n",
    "**GPT-2 używa Byte Pair Encoding (BPE)** → dzieli tekst na subtokeny, a nie słowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d171f-220f-46e9-8286-39a755bc0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"sztuczna\", \"inteligencja\", \"przyszłość\"]\n",
    "\n",
    "# Tokeny GPT-2\n",
    "tokens = [tokenizer.encode(w, add_special_tokens=False) for w in words]\n",
    "print(\"Tokeny:\", tokens)\n",
    "\n",
    "# Embeddingi dla tokenów\n",
    "embeddings = model.transformer.wte.weight  # wte = word token embeddings\n",
    "for i, w_tokens in enumerate(tokens):\n",
    "    for t in w_tokens:\n",
    "        print(f\"Słowo: {words[i]}, Token: {t}, Embedding shape: {embeddings[t].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94321154-2a59-4c2f-af6a-6d9aa3d12376",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Sztuczna inteligencja\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Forward pass, bez generacji\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
    "\n",
    "# Softmax dla ostatniego tokena\n",
    "import torch.nn.functional as F\n",
    "last_token_logits = logits[0, -1, :]\n",
    "probs = F.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "# 100 najbardziej prawdopodobnych tokenów\n",
    "top_k = torch.topk(probs, 100)\n",
    "for token_id, prob in zip(top_k.indices.tolist(), top_k.values.tolist()):\n",
    "    print(tokenizer.decode([token_id]), prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fc916-67ed-44de-9dc2-96647b6916c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(100), top_k.values.cpu().numpy())\n",
    "plt.title(\"Top 100 prawdopodobnych tokenów\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdae029-7606-4cff-bf98-acb8429c3ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
