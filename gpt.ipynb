{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf10e26-4608-4cbf-a915-1c76407abbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: fastapi in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (0.128.0)\n",
      "Requirement already satisfied: uvicorn in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (0.39.0)\n",
      "Requirement already satisfied: nest-asyncio in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from triton==3.4.0->torch) (73.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from triton==3.4.0->torch) (8.5.0)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from fastapi) (0.49.3)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from fastapi) (2.12.5)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from fastapi) (0.0.4)\n",
      "Requirement already satisfied: click>=7.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from pydantic>=2.7.0->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from pydantic>=2.7.0->fastapi) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from pydantic>=2.7.0->fastapi) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (1.2.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/wsl/.conda/envs/sage/lib/python3.9/site-packages (from importlib-metadata->triton==3.4.0->torch) (3.20.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pobranie bibliotek oraz modelu\n",
    "!pip install transformers torch fastapi uvicorn nest-asyncio\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name=\"gpt2\"\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, resume_download=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, resume_download=True)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddffd5a9-cea4-4034-9f97-8d513c441956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [9004]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:63564 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:63564 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:63567 - \"GET /generate?prompt=Hej HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [9004]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m config\u001b[38;5;241m=\u001b[39muvicorn\u001b[38;5;241m.\u001b[39mConfig(app\u001b[38;5;241m=\u001b[39mapp,host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,log_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m server\u001b[38;5;241m=\u001b[39muvicorn\u001b[38;5;241m.\u001b[39mServer(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m---> 20\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sage/lib/python3.9/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sage/lib/python3.9/site-packages/nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/sage/lib/python3.9/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sage/lib/python3.9/asyncio/tasks.py:339\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sage/lib/python3.9/asyncio/tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/.conda/envs/sage/lib/python3.9/site-packages/uvicorn/server.py:71\u001b[0m, in \u001b[0;36mServer.serve\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture_signals():\n\u001b[0;32m---> 71\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serve(sockets)\n",
      "File \u001b[0;32m~/.conda/envs/sage/lib/python3.9/contextlib.py:126\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sage/lib/python3.9/site-packages/uvicorn/server.py:331\u001b[0m, in \u001b[0;36mServer.capture_signals\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# If we did gracefully shut down due to a signal, try to\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# trigger the expected behaviour now; multiple signals would be\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# done LIFO, see https://stackoverflow.com/questions/48434964\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m captured_signal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_captured_signals):\n\u001b[0;32m--> 331\u001b[0m     \u001b[43msignal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaptured_signal\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI #framework do tworzenia REST api (m.in. obsługuje żądania HTTP) | FastAPI tworzy aplikacje serwerową\n",
    "import nest_asyncio #pozwala uruchomić asynchroniczny kod w środowisku np. Jupyter\n",
    "import uvicorn #Serwer ASGI (Asynchronous Server Gateway Interface), czyli serwer, który obsługuje FastAPI. To on nasłuchuje na porcie i odpowiada na żądania HTTP.\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()  # potrzebne w Jupyterze\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/generate\")\n",
    "def generate_text(prompt: str, max_length: int = 50):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_length=max_length, do_sample=True)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"generated_text\": text}\n",
    "\n",
    "config=uvicorn.Config(app=app,host=\"127.0.0.1\",port=5000,log_level=\"info\")\n",
    "server=uvicorn.Server(config=config)\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(server.serve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d171f-220f-46e9-8286-39a755bc0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"sztuczna\", \"inteligencja\", \"przyszłość\"]\n",
    "\n",
    "# Tokeny GPT-2\n",
    "tokens = [tokenizer.encode(w, add_special_tokens=False) for w in words]\n",
    "print(\"Tokeny:\", tokens)\n",
    "\n",
    "# Embeddingi dla tokenów\n",
    "embeddings = model.transformer.wte.weight  # wte = word token embeddings\n",
    "for i, w_tokens in enumerate(tokens):\n",
    "    for t in w_tokens:\n",
    "        print(f\"Słowo: {words[i]}, Token: {t}, Embedding shape: {embeddings[t].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94321154-2a59-4c2f-af6a-6d9aa3d12376",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Sztuczna inteligencja\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Forward pass, bez generacji\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
    "\n",
    "# Softmax dla ostatniego tokena\n",
    "import torch.nn.functional as F\n",
    "last_token_logits = logits[0, -1, :]\n",
    "probs = F.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "# 100 najbardziej prawdopodobnych tokenów\n",
    "top_k = torch.topk(probs, 100)\n",
    "for token_id, prob in zip(top_k.indices.tolist(), top_k.values.tolist()):\n",
    "    print(tokenizer.decode([token_id]), prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fc916-67ed-44de-9dc2-96647b6916c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(100), top_k.values.cpu().numpy())\n",
    "plt.title(\"Top 100 prawdopodobnych tokenów\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
